{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crude-subcommittee",
   "metadata": {
    "id": "jpIfXmbjCc9G"
   },
   "source": [
    "# TigerGraph AMLSim Demo\n",
    "\n",
    "To Get Started you will need an instance of TigerGraph running. The fastest way to get a box running is using **https://tgcloud.io**. \n",
    "\n",
    "If it's your first time using the cloud portal checkout [**Getting Started with TigerGraph 3.0**](https://www.tigergraph.com/blog/getting-started-with-tigergraph-3-0/)\n",
    "\n",
    "## Installing AMLSim Graph\n",
    "\n",
    "To use the graph solution shown in this demo you will want to **import an exsisting solution** which is located inside this repository called `AMLSim_3_0_6.gz`. Once the solution is imported add the data files to the cloud server by following the Getting Started with TigerGraph 3.0 blog. The last step is to install the default queries that came with the starter kit. These instruction are also in the blog mentioned above.\n",
    "\n",
    "## About AMLSim\n",
    "The AMLSim project is intended to provide a multi-agent based simulator that generates synthetic banking transaction data together with a set of known money laundering patterns - mainly for the purpose of testing machine learning models and graph algorithms.\n",
    "\n",
    "**AMLSim Official GitHub**\n",
    "https://github.com/IBM/AMLSim\n",
    "\n",
    "**AMLSim WIKI** https://github.com/IBM/AMLSim/wiki\n",
    "\n",
    "**AMLSim Data** https://github.com/IBM/AMLSim/wiki/Download-Example-Data-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-monroe",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "\n",
    "Let's take a quick look at the data that we will be using for this lab. To do this we can simply convert our csv files that come with this lab and turn them into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/home/ec2-user/SageMaker/AMLSim_Python_Lab/data/accounts.csv')\n",
    "df2 = pd.read_csv('/home/ec2-user/SageMaker/AMLSim_Python_Lab/data/alerts.csv')\n",
    "df3 = pd.read_csv('/home/ec2-user/SageMaker/AMLSim_Python_Lab/data/transactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-palestinian",
   "metadata": {},
   "source": [
    "The first file has information about Customers and Accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-bikini",
   "metadata": {},
   "source": [
    "The second file includes information regarding the labeled fraud transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-crazy",
   "metadata": {},
   "source": [
    "Our last file includes all the transactions with amounts, timestamps, sender, receiver, and ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-melissa",
   "metadata": {},
   "source": [
    "### Installing and Importing packages\n",
    "\n",
    "For this lab the critical package we will need is the `pyTigerGraph` package which is community built python connector. If you would like to use the `beta` version of the package that is acceptable to. The beta version includes the latest features that are being developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyTigerGraphBeta\n",
    "!pip install flat-table\n",
    "\n",
    "import pyTigerGraphBeta as tg\n",
    "import flat_table\n",
    "import IPython.display as disp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-guarantee",
   "metadata": {},
   "source": [
    "After we have our packages we will want to provide a few details on our box. You will need to fill in the parameters below according to your provisioned solution. If your unfamiliar with how to generate a secret check out this short [blog](https://towardsdatascience.com/generating-a-secret-in-tigergraph-e5139d52dff6) on a step by step walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = tg.TigerGraphConnection(host=\"https://aml-sim-sagemaker.i.tgcloud.io\", username=\"tigergraph\", password=\"tigergraph\", graphname = \"AMLSim\")\n",
    "conn.apiToken = conn.getToken(\"0dqle85rg436lg25qabtki0lqenunfvj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-being",
   "metadata": {},
   "source": [
    "Let's conduct a simple test to see if everything works. We will make a call and fetch the endpoints that are available for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = conn.getEndpoints()\n",
    "disp.JSON(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-niagara",
   "metadata": {},
   "source": [
    "Let's call a few of those endpoints that are included in this package. Now remember every time you create and install a GQSL query, that query get's compiled and exposed as a rest service making it VERY easy to interact with TigerGraph. Let's test it out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = conn.runInstalledQuery(\"accountInfo\", {\"limit_x\":\"2\"})\n",
    "disp.JSON(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-occasions",
   "metadata": {},
   "source": [
    "Take note that it looks like we are missing details about users. That is because we haven't generated those attributes yet. We can simply generate those new features by running the `accountActivity` query that was provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conn.runInstalledQuery(\"accountActivity\", {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-bowling",
   "metadata": {},
   "source": [
    "Let's re-run the query and see the features that were generated. Perfect. We have everything except a few attributes called `label` and `pagerank`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = conn.runInstalledQuery(\"accountInfo\", {\"limit_x\":\"2\"})\n",
    "disp.JSON(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-smoke",
   "metadata": {},
   "source": [
    "## Using Graph Structures to Generate new Features using Algorithms\n",
    "#### PageRank\n",
    "**Description**\n",
    "\n",
    "The PageRank algorithm measures the influence of each vertex on every other vertex. PageRank influence is defined recursively: a vertex's influence is based on the influence of the vertices which refer to it. A vertex's influence tends to increase if (1) it has more referring vertices or if (2) its referring vertices have higher influence. The analogy to social influence is clear.\n",
    "\n",
    "[Full detailed explanation](https://docs.tigergraph.com/tigergraph-platform-overview/graph-algorithm-library#pagerank)\n",
    "\n",
    "#### Label Propagation\n",
    "**Description**\n",
    "\n",
    "Label Propagation is a heuristic method for determining communities.  The idea is simple: If the plurality of your neighbors all bear the label X, then you should label yourself as also a member of X. The algorithm begins with each vertex having its own unique label. Then we iteratively update labels based on the neighbor influence described above. It is important that the order for updating the vertices be random. The algorithm is favored for its efficiency and simplicity, but it is not guaranteed to produce the same results every time.\n",
    "In a variant version, some vertices could initially be known to belong to the same community. If they are well-connected to one another, they are likely to preserve their common membership and influence their neighbors,\n",
    "\n",
    "[Full detailed explanation](https://docs.tigergraph.com/tigergraph-platform-overview/graph-algorithm-library#label-propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-therapist",
   "metadata": {},
   "source": [
    "### Installing Queries\n",
    "\n",
    "To load these Algorithms into our TigerGraph Cloud instance we will use the Python Extension called [tgcloud-jupyter 0.9.6](https://pypi.org/project/tgcloud-jupyter/). This extension can be added to your JuypterLab by running `pip install tgcloud-jupyter`. If you would like to manually create these queries you can fetch the queries from [https://github.com/tigergraph/gsql-graph-algorithms](https://github.com/tigergraph/gsql-graph-algorithms). \n",
    "\n",
    "If the extension isn't showing after running the install you may need to execute this command `jlpm && jlpm build && jupyter-labextension link .` in the extension directory to rebuild extenstions. You extention should be located in `/home/ec2-user/anaconda3/envs/JupyterSystemEnv/share/jupyter/labextensions/tgcloud-jupyter`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-weekend",
   "metadata": {},
   "source": [
    "### Running Queries\n",
    "\n",
    "Once those queries are installed let's execute them to generate those new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conn.runInstalledQuery(\"pageRank\", {\"v_type\":\"Account\", \"e_type\":\"Send_To\", \"max_change\":\"0.001\", \"max_iter\":\"25\", \"damping\":\"0.85\", \"top_k\":\"10\", \"print_accum\":\"TRUE\", \"result_attr\":\"pagerank\", \"file_path\":\"\", \"display_edges\":\"FALSE\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_prop = conn.runInstalledQuery(\"label_prop\", {\"v_type\":\"Account\", \"e_type\":\"Send_To\", \"max_iter\":\"10\", \"output_limit\":\"10\", \"print_accum\":\"TRUE\", \"file_path\":\"\", \"attr\":\"label\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-response",
   "metadata": {},
   "source": [
    "Let's take a look at some account information to see if everything looks like it was generated properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = conn.runInstalledQuery(\"accountInfo\", {\"limit_x\":\"2\"})\n",
    "disp.JSON(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-grammar",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "Now that we've generated a few interesting features let's grab those transactions to use for training a fraud model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_hop = conn.runInstalledQuery(\"txMultiHopLimit\", {}, timeout=\"10000000000000000\", sizeLimit=\"1500000000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-advocate",
   "metadata": {},
   "source": [
    "Let's convert that JSON into a dataframe to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tx_hop = pd.DataFrame(tx_hop[0][\"@@txRecords\"])\n",
    "df_tx_hop = flat_table.normalize(df_tx_hop)\n",
    "df_tx_hop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-livestock",
   "metadata": {},
   "source": [
    "Next let's save a that data into a tmp folder so we can send it to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tx_hop.to_csv(r'./tmp/20210412AMLsim.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-extra",
   "metadata": {},
   "source": [
    "# SageMaker Model Generation \n",
    "\n",
    "During this part of the lab we will be working with a few of the AWS services. To make use these services you must have an execution role that can be used with this notebook session. \n",
    "\n",
    "First let's grab a few SageMaker packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-avenue",
   "metadata": {},
   "source": [
    "Next let's assign this session to a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-bradley",
   "metadata": {},
   "source": [
    "Also, let's assign our execution role to the parameter role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-offense",
   "metadata": {},
   "source": [
    "Test to see if it was assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-dynamics",
   "metadata": {},
   "source": [
    "## Upload Data to S3 for Training\n",
    "\n",
    "Next we will be uploading our data to S3. Donâ€™t worry this is extremely simple. All you need to do is use the `upload_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = sagemaker_session.upload_data(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-officer",
   "metadata": {},
   "source": [
    "Test to see if it was uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-nylon",
   "metadata": {},
   "source": [
    "## Create SageMaker Scikit Estimator\n",
    "\n",
    "Next we will pass our `fraud-prediction.py` script which includes our training and model generation information. Let's explore that script by navigating to the `fraud-prediction.py`.\n",
    "\n",
    "Once we have our script ready we will assign that script as our `entry_point`. You will also want to fill out a few more parameters below depending on what your requirements are needed. \n",
    "\n",
    "This information includes the `role` and `session` parameters that we generated above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'fraud-prediction.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    framework_version=\"0.20.0\",\n",
    "    py_version=\"py3\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-dressing",
   "metadata": {},
   "source": [
    "## Train SKLearn Estimator on AML Data\n",
    "\n",
    "Once we have everything setup let's kick off the training process. This will create an instance based of the parameters you provided above. It will then train a model and save that model for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-medium",
   "metadata": {},
   "source": [
    "## Deploy the model \n",
    "\n",
    "On the last step. Once you have a model generated let's now deploy that model in AWS. Simply use the `deploy` command and pass information about the instance you would like to deploy the model on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-survival",
   "metadata": {},
   "source": [
    "If all went well you will now see your endpoint that was generated and can now use it for predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-warrior",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You've now not only graph based features using algorithiums, but you've extracted those features, created an S3 bucket, deployed a training server, created a machine learning model using the training server, and lastly deployed that model into an endpoint!!\n",
    "\n",
    "#### Where to go if you would like help with this tutorial?\n",
    "\n",
    "[TigerGraphs Community Forum](https://community.tigergraph.com/) simply ask questions by creating a new topic or replying to an existing topic.\n",
    "\n",
    "[TigerGraph's Developer Chart](https://discord.gg/F2c9b9v) talk with other TigerGraph developers across the world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
